{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "347441e8-0653-49ef-8a4a-48a5ba94d55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcdd697e-9aca-40d7-83dd-a724552f9f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'CLANNAD_LLaMA'...\n",
      "remote: Enumerating objects: 25, done.\u001b[K\n",
      "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
      "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
      "remote: Total 25 (delta 2), reused 21 (delta 2), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (25/25), 1.52 MiB | 621.00 KiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/SUTFutureCoder/CLANNAD_LLaMA.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb5768d-91eb-440f-bc6d-a7bcd207c00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ymcui/Chinese-LLaMA-Alpaca-2.git\n",
    "!pip install -r Chinese-LLaMA-Alpaca-2/requirements.txt\n",
    "!pip install gradio\n",
    "!pip install datasets\n",
    "!pip install scikit-learn\n",
    "!pip install deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96f27c05-09e6-45dc-8ce9-5729b5e4fcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-09-16 18:29:31--  https://github.com/git-lfs/git-lfs/releases/download/v3.0.1/git-lfs-linux-amd64-v3.0.1.tar.gz\n",
      "Connecting to 192.168.126.12:12798... connected.\n",
      "Proxy request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/13021798/b3c87ba7-a45d-4f58-b3c0-b9e9544dfcc0?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230916%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230916T102932Z&X-Amz-Expires=300&X-Amz-Signature=6a216dbed3cef97b0d06ede84ed6a85fe8a288f643ad637598d9efa5050a2c5f&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=13021798&response-content-disposition=attachment%3B%20filename%3Dgit-lfs-linux-amd64-v3.0.1.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2023-09-16 18:29:32--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/13021798/b3c87ba7-a45d-4f58-b3c0-b9e9544dfcc0?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230916%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230916T102932Z&X-Amz-Expires=300&X-Amz-Signature=6a216dbed3cef97b0d06ede84ed6a85fe8a288f643ad637598d9efa5050a2c5f&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=13021798&response-content-disposition=attachment%3B%20filename%3Dgit-lfs-linux-amd64-v3.0.1.tar.gz&response-content-type=application%2Foctet-stream\n",
      "Connecting to 192.168.126.12:12798... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 4062952 (3.9M) [application/octet-stream]\n",
      "Saving to: ‘git-lfs-linux-amd64-v3.0.1.tar.gz.1’\n",
      "\n",
      "git-lfs-linux-amd64 100%[===================>]   3.87M  1.72MB/s    in 2.2s    \n",
      "\n",
      "2023-09-16 18:29:36 (1.72 MB/s) - ‘git-lfs-linux-amd64-v3.0.1.tar.gz.1’ saved [4062952/4062952]\n",
      "\n",
      "README.md\n",
      "CHANGELOG.md\n",
      "man/\n",
      "man/git-lfs-pull.1.html\n",
      "man/git-lfs-fsck.1\n",
      "man/git-lfs-filter-process.1\n",
      "man/git-lfs-pointer.1\n",
      "man/git-lfs-track.1\n",
      "man/git-lfs.1\n",
      "man/git-lfs-pre-push.1.html\n",
      "man/git-lfs-status.1\n",
      "man/git-lfs-update.1.html\n",
      "man/git-lfs-install.1\n",
      "man/git-lfs-smudge.1\n",
      "man/git-lfs-push.1.html\n",
      "man/git-lfs-locks.1\n",
      "man/git-lfs-install.1.html\n",
      "man/git-lfs-locks.1.html\n",
      "man/git-lfs-env.1.html\n",
      "man/git-lfs-post-merge.1.html\n",
      "man/git-lfs-logs.1\n",
      "man/git-lfs-untrack.1.html\n",
      "man/git-lfs-clone.1.html\n",
      "man/git-lfs-update.1\n",
      "man/git-lfs-post-commit.1.html\n",
      "man/git-lfs-config.5.html\n",
      "man/git-lfs-lock.1.html\n",
      "man/git-lfs-ls-files.1\n",
      "man/git-lfs-env.1\n",
      "man/git-lfs-ls-files.1.html\n",
      "man/git-lfs-clean.1.html\n",
      "man/git-lfs-prune.1\n",
      "man/git-lfs-ext.1.html\n",
      "man/git-lfs-config.5\n",
      "man/git-lfs-fetch.1\n",
      "man/git-lfs-smudge.1.html\n",
      "man/git-lfs-filter-process.1.html\n",
      "man/git-lfs-push.1\n",
      "man/git-lfs-post-commit.1\n",
      "man/git-lfs-fetch.1.html\n",
      "man/git-lfs-logs.1.html\n",
      "man/git-lfs-migrate.1.html\n",
      "man/git-lfs-pointer.1.html\n",
      "man/git-lfs-checkout.1\n",
      "man/git-lfs-post-merge.1\n",
      "man/git-lfs-uninstall.1.html\n",
      "man/git-lfs-post-checkout.1\n",
      "man/git-lfs.1.html\n",
      "man/git-lfs-post-checkout.1.html\n",
      "man/git-lfs-status.1.html\n",
      "man/git-lfs-track.1.html\n",
      "man/git-lfs-uninstall.1\n",
      "man/git-lfs-untrack.1\n",
      "man/git-lfs-clone.1\n",
      "man/git-lfs-pre-push.1\n",
      "man/git-lfs-unlock.1.html\n",
      "man/git-lfs-fsck.1.html\n",
      "man/git-lfs-pull.1\n",
      "man/git-lfs-ext.1\n",
      "man/git-lfs-unlock.1\n",
      "man/git-lfs-clean.1\n",
      "man/git-lfs-checkout.1.html\n",
      "man/git-lfs-prune.1.html\n",
      "man/git-lfs-lock.1\n",
      "man/git-lfs-migrate.1\n",
      "git-lfs\n",
      "install.sh\n",
      "Git LFS initialized.\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/git-lfs/git-lfs/releases/download/v3.0.1/git-lfs-linux-amd64-v3.0.1.tar.gz\n",
    "!tar -xzvf git-lfs-linux-amd64-v3.0.1.tar.gz\n",
    "!./install.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f97477-6203-472c-87f5-72555c59c632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: 'git lfs clone' is deprecated and will not be updated\n",
      "          with new flags from 'git clone'\n",
      "\n",
      "'git clone' has been updated in upstream Git to have comparable\n",
      "speeds to 'git lfs clone'.\n",
      "Cloning into 'chinese-alpaca-2-7b'...\n",
      "remote: Enumerating objects: 33, done.\u001b[K\n",
      "remote: Total 33 (delta 0), reused 0 (delta 0), pack-reused 33\u001b[K\n",
      "Unpacking objects: 100% (33/33), 6.17 KiB | 789.00 KiB/s, done.\n",
      "Downloading LFS objects:  33% (1/3), 43 MB | 619 KB/s                           \r"
     ]
    }
   ],
   "source": [
    "!cd /root/autodl-tmp && git lfs clone https://huggingface.co/ziqingyang/chinese-alpaca-2-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3396b1aa-3d36-46ff-abfe-0a789c9fa57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xformers\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7ded070-afe4-4f12-b69f-feff0e0d68c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Install the latest transformers (commit equal or later than d533465) to enable CFG sampling.\n",
      "USE_MEM_EFF_ATTENTION:  True\n",
      "STORE_KV_BEFORE_ROPE: False\n",
      "Apply NTK scaling with ALPHA=1.0\n",
      "The value of scaling factor will be read from model config file, or set to 1.\n",
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:12<00:00,  6.31s/it]\n",
      "Vocab of the base model: 55296\n",
      "Vocab of the tokenizer: 55296\n",
      "../Chinese-LLaMA-Alpaca-2/scripts/inference/gradio_demo.py:477: GradioDeprecationWarning: The `style` method is deprecated. Please set these arguments in the constructor instead.\n",
      "  system_prompt_input = gr.Textbox(\n",
      "../Chinese-LLaMA-Alpaca-2/scripts/inference/gradio_demo.py:483: GradioDeprecationWarning: The `style` method is deprecated. Please set these arguments in the constructor instead.\n",
      "  negative_prompt_input = gr.Textbox(\n",
      "../Chinese-LLaMA-Alpaca-2/scripts/inference/gradio_demo.py:491: GradioDeprecationWarning: The `style` method is deprecated. Please set these arguments in the constructor instead.\n",
      "  user_input = gr.Textbox(\n",
      "Running on local URL:  http://0.0.0.0:19324\n",
      "Running on public URL: https://fe10771835679f08f9.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
      "len(history): 1\n",
      "history:  [['hellohello', None]]\n",
      "Input length: 37\n",
      "len(history): 2\n",
      "history:  [['hellohello', 'Hello! How can I assist you today?'], ['2023年会有经济危机么', None]]\n",
      "Input length: 64\n",
      "len(history): 3\n",
      "history:  [['hellohello', 'Hello! How can I assist you today?'], ['2023年会有经济危机么', \"It is difficult to predict the future with certainty, but there are several factors that could contribute to an economic crisis in 2023. These include:\\n1. High levels of debt and deficits in many countries, which could lead to a financial crisis if interest rates increase or borrowing costs become too high.\\n2. The ongoing trade war between the US and China, which has caused global economic uncertainty and disruptions in supply chains.\\n3. Uncertainty surrounding Brexit and the UK's relationship with the EU after leaving the bloc.\\n4. Increasing tensions between major powers such as Russia and the US, which could lead to instability and economic disruption.\\n5. Climate change and its impact on agriculture, natural disasters, and energy production.\\n6. Technological changes and their effect on industries such as transportation, healthcare, and entertainment.\\n7. Political instability and social unrest in some regions, which could lead to businesses shutting down and investments being put on hold.\\nIt is important to note that these are just potential risks and not guaranteed to happen. However, it is always wise to be prepared for unexpected events and have contingencies in place should they occur.\"], ['请用中文回答', None]]\n",
      "Input length: 348\n",
      "^C\n",
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 0.0.0.0:19324 <> https://fe10771835679f08f9.gradio.live\n"
     ]
    }
   ],
   "source": [
    "!cd /root/autodl-tmp && python ../Chinese-LLaMA-Alpaca-2/scripts/inference/gradio_demo.py --base_model chinese-alpaca-2-7b --load_in_8bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0851c0-9234-4c0c-9142-46d6eba7ab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /root/Chinese-LLaMA-Alpaca-2/scripts/training && bash run_pt.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "329a7374-12fa-4fc3-b033-9f852a4f2f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Base model: /root/autodl-tmp/chinese-alpaca-2-7b\n",
      "LoRA model: /root/autodl-tmp/chinese-alpaca-2-7b-clannad/pt_lora_model\n",
      "Loading /root/autodl-tmp/chinese-alpaca-2-7b-clannad/pt_lora_model\n",
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "Loading ckpt pytorch_model-00001-of-00002.bin\n",
      "Merging...\n",
      "Saving ckpt pytorch_model-00001-of-00002.bin to /root/autodl-tmp/chinese-alpaca-2-7b-clannad-pt in HF format...\n",
      "Loading ckpt pytorch_model-00002-of-00002.bin\n",
      "Merging...\n",
      "Saving ckpt pytorch_model-00002-of-00002.bin to /root/autodl-tmp/chinese-alpaca-2-7b-clannad-pt in HF format...\n",
      "Saving tokenizer\n",
      "Saving config.json from /root/autodl-tmp/chinese-alpaca-2-7b\n",
      "Saving generation_config.json from /root/autodl-tmp/chinese-alpaca-2-7b\n",
      "Saving pytorch_model.bin.index.json from /root/autodl-tmp/chinese-alpaca-2-7b\n",
      "Done.\n",
      "Check output dir: /root/autodl-tmp/chinese-alpaca-2-7b-clannad-pt\n"
     ]
    }
   ],
   "source": [
    "!cd /root/Chinese-LLaMA-Alpaca-2 && python scripts/merge_llama2_with_chinese_lora_low_mem.py \\\n",
    "    --base_model /root/autodl-tmp/chinese-alpaca-2-7b \\\n",
    "    --lora_model /root/autodl-tmp/chinese-alpaca-2-7b-clannad/pt_lora_model \\\n",
    "    --output_type huggingface \\\n",
    "    --output_dir /root/autodl-tmp/chinese-alpaca-2-7b-clannad-pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f037d6-82b8-46ee-9d61-c9b1ee843a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /root/Chinese-LLaMA-Alpaca-2/scripts/training && bash run_sft.sh > logs.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f95d5c7-8215-4362-9e7f-05024a0684a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Base model: /root/autodl-tmp/chinese-alpaca-2-7b-clannad-pt\n",
      "LoRA model: /root/autodl-tmp/chinese-alpaca-2-7b-clannad-sft-new/sft_lora_model\n",
      "Loading /root/autodl-tmp/chinese-alpaca-2-7b-clannad-sft-new/sft_lora_model\n",
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "Loading ckpt pytorch_model-00001-of-00002.bin\n",
      "Merging...\n",
      "Saving ckpt pytorch_model-00001-of-00002.bin to /root/autodl-tmp/chinese-alpaca-2-7b-clannad-model in HF format...\n",
      "Loading ckpt pytorch_model-00002-of-00002.bin\n",
      "Merging...\n",
      "Saving ckpt pytorch_model-00002-of-00002.bin to /root/autodl-tmp/chinese-alpaca-2-7b-clannad-model in HF format...\n",
      "Saving tokenizer\n",
      "Saving config.json from /root/autodl-tmp/chinese-alpaca-2-7b-clannad-pt\n",
      "Saving generation_config.json from /root/autodl-tmp/chinese-alpaca-2-7b-clannad-pt\n",
      "Saving pytorch_model.bin.index.json from /root/autodl-tmp/chinese-alpaca-2-7b-clannad-pt\n",
      "Done.\n",
      "Check output dir: /root/autodl-tmp/chinese-alpaca-2-7b-clannad-model\n"
     ]
    }
   ],
   "source": [
    "!cd /root/Chinese-LLaMA-Alpaca-2 && python scripts/merge_llama2_with_chinese_lora_low_mem.py \\\n",
    "    --base_model /root/autodl-tmp/chinese-alpaca-2-7b-clannad-pt \\\n",
    "    --lora_model /root/autodl-tmp/chinese-alpaca-2-7b-clannad-sft-new/sft_lora_model \\\n",
    "    --output_type huggingface \\\n",
    "    --output_dir /root/autodl-tmp/chinese-alpaca-2-7b-clannad-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72800c5c-6e98-42fa-9790-59af81ea5131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Install the latest transformers (commit equal or later than d533465) to enable CFG sampling.\n",
      "USE_MEM_EFF_ATTENTION:  True\n",
      "STORE_KV_BEFORE_ROPE: False\n",
      "Apply NTK scaling with ALPHA=1.0\n",
      "The value of scaling factor will be read from model config file, or set to 1.\n",
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:13<00:00,  6.93s/it]\n",
      "Vocab of the base model: 55296\n",
      "Vocab of the tokenizer: 55296\n",
      "../Chinese-LLaMA-Alpaca-2/scripts/inference/gradio_demo.py:477: GradioDeprecationWarning: The `style` method is deprecated. Please set these arguments in the constructor instead.\n",
      "  system_prompt_input = gr.Textbox(\n",
      "../Chinese-LLaMA-Alpaca-2/scripts/inference/gradio_demo.py:483: GradioDeprecationWarning: The `style` method is deprecated. Please set these arguments in the constructor instead.\n",
      "  negative_prompt_input = gr.Textbox(\n",
      "../Chinese-LLaMA-Alpaca-2/scripts/inference/gradio_demo.py:491: GradioDeprecationWarning: The `style` method is deprecated. Please set these arguments in the constructor instead.\n",
      "  user_input = gr.Textbox(\n",
      "Running on local URL:  http://0.0.0.0:19324\n",
      "Running on public URL: https://c8f9b3e49e4ca7bbb3.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
      "len(history): 1\n",
      "history:  [['【朋也】「春原，我是谁？」', None]]\n",
      "Input length: 115\n",
      "len(history): 2\n",
      "history:  [['【朋也】「春原，我是谁？」', '【春原】「你……」'], ['【朋也】「琴美，我是谁？」', None]]\n",
      "Input length: 145\n",
      "len(history): 1\n",
      "history:  [['【朋也】「琴美，我是谁？」', None]]\n",
      "Input length: 115\n",
      "len(history): 2\n",
      "history:  [['【朋也】「琴美，我是谁？」', '【琴美】「朋也君」'], ['【朋也】「古河，我是谁？」', None]]\n",
      "Input length: 145\n",
      "len(history): 3\n",
      "history:  [['【朋也】「琴美，我是谁？」', '【琴美】「朋也君」'], ['【朋也】「古河，我是谁？」', '【古河】「冈崎同学」'], ['【朋也】「渚，我是谁？」', None]]\n",
      "Input length: 174\n"
     ]
    }
   ],
   "source": [
    "!export GRADIO_SERVER_PORT=19325 && cd /root/autodl-tmp && python ../Chinese-LLaMA-Alpaca-2/scripts/inference/gradio_demo.py --base_model chinese-alpaca-2-7b-clannad-model --load_in_8bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5a352e-e4cd-4bca-abed-543d8392e2c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
